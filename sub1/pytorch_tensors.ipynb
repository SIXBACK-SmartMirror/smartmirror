{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbdISK2ASVPb"
      },
      "source": [
        "# 과제 목표\n",
        "\n",
        "1. NumPy의 array 형의 사용법 숙지\n",
        "2. PyTorch Tensor 형의 사용법 숙지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhH_mRqgS_bz"
      },
      "source": [
        "# NumPy arrays\n",
        "\n",
        "NumPy는 행렬 연산을 쉽게 하기 위하여 자주 사용하는 파이썬 라이브러리로, 딥러닝 뿐만 아니라 데이터 분석 등 널리 사용되고 있습니다. 매우 다양한 연산자 및 기능을 지원하고 있으며, 자세한 내용은 공식 document에서 쉽게 찾아볼 수 있습니다.\n",
        "\n",
        "📌 **Install**\n",
        "\n",
        "Colab에는 기본적으로 NumPy가 설치되어 있지만, 개인 컴퓨터에서 설치한다면 `pip install numpy`로 간단히 설치할 수 있습니다.\n",
        "\n",
        "📌 **Import NumPy**\n",
        "\n",
        "Python에서 사용하기 위하여 라이브러리를 불러올 때는 `import numpy as np`로 합니다. `np`로 줄여 쓰는 것이 convention입니다.\n",
        "\n",
        "📌 앞으로 NumPy의 array는 `ndarray`라고 줄여 부르겠습니다.\n",
        "\n",
        "😎 **추가자료**\n",
        "- [NumPy Document (API References)](https://numpy.org/doc/stable/reference/index.html)\n",
        "- [NumPy 연습 문제 모음](https://github.com/rougier/numpy-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du20woZ2Zl8Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0kbgR6xZD6R"
      },
      "outputs": [],
      "source": [
        "\"\"\"array 생성하기 (기본): 원하는 차원의 리스트를 전달하여 생성 가능\"\"\"\n",
        "# 1차원\n",
        "arr1 = np.array([1, 2, 3])\n",
        "# 2차원\n",
        "arr2 = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "# 3차원\n",
        "arr3 = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXAZwQgKc2lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d03304-4613-47ec-e138-97ddfaa4e324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of arr3: (2, 2, 3)\n",
            "Type of arr3: int64\n"
          ]
        }
      ],
      "source": [
        "\"\"\"행렬 정보 확인하기\"\"\"\n",
        "print(f\"Shape of arr3: {arr3.shape}\") # 형태 (크기), output: (2, 2, 3)\n",
        "print(f\"Type of arr3: {arr3.dtype}\") # 자료형, output: int64 (colab), int32 (local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3sw_u6sc8BZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"특정 array 생성하기: 미리 정의된 메서드에 원하는 행렬 크기를 전달하여 생성 가능\"\"\"\n",
        "# 모두 0로 채운 배열\n",
        "zero_arr = np.zeros((3, 3)) # 3x3 행렬\n",
        "# 모두 1로 채운 배열\n",
        "one_arr = np.ones((2, 4)) # 2x4 행렬\n",
        "# 랜덤 수로 채운 배열 (0~1 난수)\n",
        "rand_arr = np.random.rand(5, 5) # 5x5 행렬\n",
        "# 랜덤 정수로 채운 배열\n",
        "rand_int_arr = np.random.randint(-5, 6, (2, 2)) # 2x2 행렬, -5 ~ 5 사이의 난수 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIiFdUWaxsyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5670f7-991f-4312-c333-3af6ffd67294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elements of which indices are 3 and 5: [2 7]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"인덱싱: 인덱스가 3, 5인 요소만 가져오기\"\"\"\n",
        "arr = np.array([4, 1, 6, 2, 0, 7])\n",
        "print(f\"Elements of which indices are 3 and 5: {arr[[3, 5]]}\") # output: [2, 7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-8Qae46eVMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f34a716-e1e9-47e7-aecf-2df0e48e021e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arr1 + arr2 = \n",
            " [[3 4 7]\n",
            " [1 1 5]]\n",
            "arr1 - arr2 = \n",
            " [[-1  2  1]\n",
            " [ 1  1  1]]\n",
            "Multiplication of arr1 and arr2 = \n",
            "[[17  8]\n",
            " [12  6]]\n",
            "[[17  8]\n",
            " [12  6]]\n",
            "arr7 = \n",
            " [10 20 30]\n",
            "Average of arr7:  20.0\n",
            "Transpose of arr1: (shape - (3, 2))\n",
            " [[1 1]\n",
            " [3 1]\n",
            " [4 3]]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"행렬 연산하기\"\"\"\n",
        "\n",
        "arr1 = np.random.randint(0, 5, (2, 3))\n",
        "arr2 = np.random.randint(0, 5, (2, 3))\n",
        "\n",
        "# 두 행렬의 합, 차\n",
        "arr3 = arr1 + arr2\n",
        "print(\"arr1 + arr2 = \\n\", arr3) # output: (2, 3) 행렬\n",
        "arr4 = arr1 - arr2\n",
        "print(\"arr1 - arr2 = \\n\", arr4) # output: (2, 3) 행렬\n",
        "\n",
        "# 두 행렬의 곱\n",
        "# 아래 두 함수는 3차원 이상의 행렬 곱에서 다른 결과를 보임\n",
        "# 참고: ndarray.T 는 행렬의 행과 열을 바꾸는 transpose 연산\n",
        "arr5 = np.dot(arr1, arr2.T) # 내적\n",
        "arr6 = np.matmul(arr1, arr2.T) # 행렬 곱\n",
        "print(\"Multiplication of arr1 and arr2 = \")\n",
        "print(arr5) # output: (2, 2) 행렬\n",
        "print(arr6) # output: (2, 2) 행렬\n",
        "\n",
        "# Broadcast: 행렬 * 상수를 수행하면 행렬의 각 요소에 상수를 곱하는 등 특정 값/벡터의 broadcast 연산 지원\n",
        "arr7 = np.array([1, 2, 3]) * 10\n",
        "print(\"arr7 = \\n\", arr7) # output: [10, 20, 30]\n",
        "\n",
        "# 평균 계산\n",
        "avg = arr7.mean()\n",
        "print(\"Average of arr7: \", avg) # output: 20.0\n",
        "\n",
        "# 전치\n",
        "trans = arr1.T\n",
        "print(f\"Transpose of arr1: (shape - {trans.shape})\\n\", trans) # output: (3, 2) 행렬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkie6NNFU4uW"
      },
      "source": [
        "# PyTorch Tensor\n",
        "\n",
        "Tensor는 NumPy의 array와 작동 방식과 제공하는 연산이 유사합니다. 또한 공식문서가 매우 잘 정리되어 있으니 검색하여 모르는 내용을 찾아보기 쉽습니다.\n",
        "\n",
        "📌 Install\n",
        "\n",
        "PyTorch 역시 Colab에서는 기본적으로 설치되어 있습니다. 컴퓨터에 설치 시 `pip install pytorch`로 가능합니다.\n",
        "\n",
        "📌 Import PyTorch\n",
        "\n",
        "불러올 때는 `import torch`를 기본으로 하며, 추후 다양한 API를 사용할 때마다 상세 import를 추가적으로 수행하기도 합니다.\n",
        "\n",
        "😎 **추가자료**\n",
        "- [PyTorch Document](https://pytorch.org/docs/stable/torch.html#tensors)\n",
        "- [PyTorch Official Tutorial - Introduction to PyTorch Tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)\n",
        "\n",
        "\n",
        "Local에서 GPU를 사용할 수 있다면 GPU를 사용할 수 있도록 준비해주세요.\n",
        "\n",
        "Colab에서 GPU를 사용하기 위해선 우측 상단의 '추가 연결 옵션'(RAM, 디스크 statistic 옆 화살표 버튼)에서 [런타임 유형 변경] - [하드웨어 가속기] 에서 GPU로 옵션을 변경해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShPTtpk1hFKc"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhFsK5thB23"
      },
      "outputs": [],
      "source": [
        "# Tensor 선언하기\n",
        "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 리스트 or ndarray -> tensor\n",
        "arr = np.array([1, 2, 3])\n",
        "t2 = torch.from_numpy(arr) # ndarray -> tensor\n",
        "\n",
        "# Tensor에서 다른 자료형으로 변경하기\n",
        "l = t1.tolist() # tensor -> list\n",
        "arr = t1.numpy() # tensor -> ndarray\n",
        "\n",
        "# Tensor를 연산할 디바이스 지정하기:\n",
        "# GPU라면 `device=\"cuda\"` 지정 혹은 `cuda()` 메서드\n",
        "# CPU라면 `device=\"cpu\"` 지정 혹은 `cpu()` 메서드\n",
        "t3 = torch.tensor([1, 2], device=\"cuda\")\n",
        "t3 = t3.to(\"cuda\")\n",
        "t3 = t3.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4dnkdOtegYs"
      },
      "source": [
        "간단하게 Tensor의 크기와 자료형, 처리되는 기기를 조회할 수 있습니다.\n",
        "\n",
        "참고로, PyTorch에서 데이터(Tensor)를 GPU 연산에 사용하려면 **해당 텐서를 GPU에 올려야 합니다**. 뒤에서 생성할 딥러닝 모델도 마찬가지입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEe0IIPNiyfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bf8f11-1517-4089-a395-a3b1fadd27f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([2, 4])\n",
            "Data Type: torch.float32\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(2, 4)\n",
        "\n",
        "print(f\"Shape: {tensor.shape}\") # 크기\n",
        "print(f\"Data Type: {tensor.dtype}\") # 자료형 - 따로 자료형을 지정하지 않는다면 float32가 기본\n",
        "print(f\"Device: {tensor.device}\") # 디바이스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DqtnxVKmtdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c9eb97-ab42-447b-d487-aab3ae2fa9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 텐서의 기기 변경\n",
        "tensor = tensor.to('cpu') # 또는 .cpu(), .cuda() 메서드도 있음.\n",
        "print(f\"Device: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwcRVflqUMvG"
      },
      "source": [
        "## PyToch Tensor 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMbYny_LuTdq"
      },
      "source": [
        "기본적 Tensor 사용법은 위에서 소개했습니다. 하지만 딥러닝 프로젝트를 개발할 땐 다양한 상황에서 여러 메서드를 사용하는 경우가 많습니다. 아래 각 상황에서 어떻게 문제를 해결하면 좋을지 검색을 통해 답을 완성해봅시다.\n",
        "\n",
        "- `torch.unsqueeze()`와 `torch.squeeze()`\n",
        "- `torch.view()`와 `torch.reshape()`\n",
        "- `torch.concat()`와 `torch.stack()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxr8a7LtuuAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10f9487-bd4b-4a34-ad7b-a5186da37859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:\n",
            "tensor([[9, 6, 7],\n",
            "        [2, 3, 2]])\n",
            "b:\n",
            "tensor([[[9, 6, 7],\n",
            "         [2, 3, 2]]])\n",
            "------------------------------\n",
            "a:\n",
            "tensor([[[4, 7, 0]],\n",
            "\n",
            "        [[7, 0, 5]]])\n",
            "b:\n",
            "tensor([[4, 7, 0],\n",
            "        [7, 0, 5]])\n",
            "------------------------------\n",
            "a:\n",
            "tensor([[[4, 1, 2],\n",
            "         [4, 8, 3]],\n",
            "\n",
            "        [[2, 3, 6],\n",
            "         [7, 1, 2]],\n",
            "\n",
            "        [[6, 4, 2],\n",
            "         [4, 2, 9]],\n",
            "\n",
            "        [[5, 2, 1],\n",
            "         [4, 5, 8]]])\n",
            "b:\n",
            "tensor([[4, 1, 2, 4, 8, 3],\n",
            "        [2, 3, 6, 7, 1, 2],\n",
            "        [6, 4, 2, 4, 2, 9],\n",
            "        [5, 2, 1, 4, 5, 8]])\n",
            "------------------------------\n",
            "a:\n",
            "tensor([[[6, 5, 0],\n",
            "         [4, 2, 5]],\n",
            "\n",
            "        [[5, 2, 3],\n",
            "         [9, 6, 5]],\n",
            "\n",
            "        [[0, 2, 0],\n",
            "         [4, 7, 8]],\n",
            "\n",
            "        [[1, 4, 3],\n",
            "         [5, 4, 8]]])\n",
            "b:\n",
            "tensor([[[6, 5, 0, 4],\n",
            "         [2, 5, 5, 2]],\n",
            "\n",
            "        [[3, 9, 6, 5],\n",
            "         [0, 2, 0, 4]],\n",
            "\n",
            "        [[7, 8, 1, 4],\n",
            "         [3, 5, 4, 8]]])\n",
            "------------------------------\n",
            "a:\n",
            "tensor([[5, 5, 1],\n",
            "        [8, 7, 7]])\n",
            "b:\n",
            "tensor([[0, 3, 1],\n",
            "        [3, 2, 7]])\n",
            "c:\n",
            "tensor([[5, 5, 1, 0, 3, 1],\n",
            "        [8, 7, 7, 3, 2, 7]])\n",
            "------------------------------\n",
            "a:\n",
            "tensor([[3, 8, 4],\n",
            "        [9, 4, 9]])\n",
            "b:\n",
            "tensor([[1, 1],\n",
            "        [3, 9],\n",
            "        [6, 9]])\n",
            "c:\n",
            "tensor([[ 51, 111],\n",
            "        [ 75, 126]])\n",
            "------------------------------\n",
            "c:\n",
            "tensor([[ 51, 111],\n",
            "        [ 75, 126]])\n",
            "d:\n",
            "tensor([[7, 8],\n",
            "        [2, 5]])\n",
            "e:\n",
            "tensor([[ 58, 119],\n",
            "        [ 77, 131]])\n",
            "------------------------------\n",
            "e:\n",
            "tensor([[ 58, 119],\n",
            "        [ 77, 131]])\n",
            "f:\n",
            "tensor([2, 4])\n",
            "g:\n",
            "Write your code\n",
            "------------------------------\n",
            "g:\n",
            "Write your code\n",
            "h:\n",
            "3\n",
            "i:\n",
            "Write your code\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "############################################################################\n",
        "# Req 1-1: PyTorch Tensor 다루기                                            #\n",
        "############################################################################\n",
        "\n",
        "################################################################################\n",
        "# TODO: 아래의 문제를 해결하며 목표한 형태 혹은 값의 행렬 연산을 수행함                  #\n",
        "################################################################################\n",
        "\n",
        "# Q1. [2, 3] 형태의 Tensor를 [1, 2, 3] 형태로 변경하기\n",
        "a = torch.randint(10, (2, 3))\n",
        "b = a.unsqueeze(0)\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q2. [2, 1, 3] 형태의 Tensor를 [2, 3] 형태로 변경하기\n",
        "a = torch.randint(10, (2, 1, 3))\n",
        "b = a.squeeze(1)\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q3. [4, 2, 3] 형태의 Tensor를 [4, 6] 형태로 변경하기\n",
        "a = torch.randint(10, (4, 2, 3))\n",
        "b = a.reshape(4, 6)\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q3. [4, 2, 3] 형태의 Tensor를 [3, 2, 4] 형태로 변경하기\n",
        "a = torch.randint(10, (4, 2, 3))\n",
        "b = a.reshape(3, 2, 4)\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q4. [2, 3] 형태의 두 텐서(혹은 리스트 등)을 [2, 6] 형태로 붙이기\n",
        "a = torch.randint(10, (2, 3))\n",
        "b = torch.randint(10, (2, 3))\n",
        "c = torch.cat((a, b), dim=1)\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\\nc:\\n{c}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q5. 2x3 행렬과 3x2 행렬의 곱하기 => 2x2 행렬\n",
        "a = torch.randint(10, (2, 3))\n",
        "b = torch.randint(10, (3, 2))\n",
        "c = torch.matmul(a, b)\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\\nc:\\n{c}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q6. 1번에서 계산한 2x2 행렬에 또다른 2x2 행렬 더하기\n",
        "d = torch.randint(10, (2, 2))\n",
        "e = c + d\n",
        "print(f\"c:\\n{c}\\nd:\\n{d}\\ne:\\n{e}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q7. 2번에서 계산한 2x2 행렬 각 열에 길이가 2인 열벡터 더하기\n",
        "f = torch.randint(10, (2,))\n",
        "g = \"\"\"Write your code\"\"\"\n",
        "print(f\"e:\\n{e}\\nf:\\n{f}\\ng:\\n{g}\")\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Q8. Q3에서 계산한 2x2 행렬 각 원소에 스칼라 값 곱하기\n",
        "h = 3\n",
        "i = \"\"\"Write your code\"\"\"\n",
        "print(f\"g:\\n{g}\\nh:\\n{h}\\ni:\\n{i}\")\n",
        "print(\"-\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZJLq1T9ZPxn"
      },
      "source": [
        "모델의 학습은 모델 파라미터를 조금씩 변경하며 이뤄집니다. 그 과정에서 학습 오차(loss)를 역전파하며 파라미터 업데이트 값을 결정합니다. 경사하강법(gradient descent)은 오차를 함수로 표현하고 미분하여 그 기울기(gradient)를 구해, 오차의 최솟값 방향으로 파라미터를 업데이트합니다.\n",
        "\n",
        "PyTorch에서는 미분 계산이 자동적으로 이뤄집니다. 모델 파라미터를 담고 있는 텐서에 자동미분을 하도록 설정하면, 해당 텐서에 대한 미분값(gradient)을 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW4OGRmdZ9-W"
      },
      "outputs": [],
      "source": [
        "# requires_grad 를 설정하면 w에 대한 미분값이 저장됨\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# 테스트를 위한 임의의 수식 정의\n",
        "x = (w*3)**2\n",
        "\n",
        "# x를 w로 미분하기: backward() 메서드로 최근 연산부터 차례대로 미분 진행\n",
        "x.backward()\n",
        "\n",
        "# 미분값 조회\n",
        "print(w.grad)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}