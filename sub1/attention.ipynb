{"cells":[{"cell_type":"markdown","metadata":{"id":"6AqCKIFAwK1-"},"source":["## 과제 목표\n","1. Transformer의 multi-head self-attention의 이해 및 구현"]},{"cell_type":"markdown","metadata":{"id":"gMRGE9TzwK2A"},"source":["# 환경 세팅"]},{"cell_type":"markdown","metadata":{"id":"UCYRwYamwK2B"},"source":["본 실습에서 사용할 라이브러리를 import 하겠습니다."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"HQL3hEx55x0J","executionInfo":{"status":"ok","timestamp":1724997555816,"user_tz":-540,"elapsed":519,"user":{"displayName":"김아름","userId":"12348658311446229205"}}},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","\n","import torch\n","import math"]},{"cell_type":"markdown","metadata":{"id":"YbsJPQCQwK2C"},"source":["# 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"e1hQFJUy6dMl"},"source":["본 실습에서 사용할 가상의 데이터를 만들겠습니다. 총 10개 시퀀스의 데이터가 이미 토큰화가 진행되어 주어졌다 가정하겠습니다."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"01KXLeH16cUF","executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":11,"user":{"displayName":"김아름","userId":"12348658311446229205"}}},"outputs":[],"source":["data = [\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n","  [60, 96, 51, 32, 90],\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n","  [75, 51],\n","  [66, 88, 98, 47],\n","  [21, 39, 10, 64, 21],\n","  [98],\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n","]"]},{"cell_type":"markdown","metadata":{"id":"jpWGRVYPwK2C"},"source":["## 1. Padding 함수 구현하기"]},{"cell_type":"markdown","metadata":{"id":"yL10MTzj6Vja"},"source":["어텐션 연산에 들어갈 데이터의 전처리를 진행하겠습니다. 본 실습의 전처리 과정에서는 데이터의 길이를 균일하게 맞출 padding 함수를 구현합니다.\n","\n","배치 내 데이터 중 최대 길이에 맞도록 다른 데이터의 마지막에 임의의 패딩 값을 패딩해 길이를 맞추는 함수입니다."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"U3i0HSml6wIR","executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":10,"user":{"displayName":"김아름","userId":"12348658311446229205"}}},"outputs":[],"source":["############################################################################\n","# Req 3-1: Padding 함수 구현하기                                             #\n","############################################################################\n","\n","def padding(data, pad_value=0):\n","    \"\"\"\n","    Args:\n","        data (list[list[int]]): data sequence\n","        pad_value (int): value to pad\n","    Returns:\n","        data (list[list[int]]): padded data sequence\n","        max_len (int): maximum length of intput data\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 배치 내 데이터 중 최대 길이에 맞추어 다른 데이터의 뒷부분에 패딩 값을 붙여        #\n","    # 길이를 동일하게 맞추어 반환함                                                     #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    # 1) 가장 긴 데이터의 길이 추출\n","    # 2) 모든 데이터를 돌면서 1에서 구한 길이에 맞게 뒷부분을 pad_value로 채우기\n","\n","    # 1) 가장 긴 데이터의 길이 추출\n","    max_len = max(len(seq) for seq in data)\n","    # 2) 모든 데이터를 돌면서 1에서 구한 길이에 맞게 뒷부분을 pad_value로 채우기\n","    for i in range(len(data)):\n","        current_len = len(data[i])\n","        if current_len < max_len:\n","            # 패딩을 추가하여 길이를 맞춤\n","            data[i] += [pad_value] * (max_len - current_len)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################\n","\n","    return data, max_len"]},{"cell_type":"markdown","metadata":{"id":"2xX5SRTCwK2D"},"source":["아래 코드로 패딩 결과를 확인할 수 있습니다."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"mO6N-pYJ7s9O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":10,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"979d6cd8-bcd1-42d6-df57-b01ca36d2ad2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 20\n","[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0]\n","[60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10]\n","[70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0]\n","[20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]\n"]}],"source":["pad_value = 0 # 패딩(Padding)은 제로(zero) 패딩으로, 빈 공간을 0으로 채움.\n","data, max_len = padding(data)\n","print(f\"Maximum sequence length: {max_len}\")\n","for d in data:\n","    print(d)"]},{"cell_type":"markdown","metadata":{"id":"6SapaNXN6N4Z"},"source":["# Single-head Self-Attention 구현하기"]},{"cell_type":"markdown","metadata":{"id":"ZP-CmVN4wK2D"},"source":["Multi-head attention을 구현하기 앞서, 먼저 single-head attention을 구현해보겠습니다. 아래는 single-head self-attention을 위한 설정 값입니다."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"nUnX16iJwK2E","executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":8,"user":{"displayName":"김아름","userId":"12348658311446229205"}}},"outputs":[],"source":["# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n","vocab_size = 10000  # 어휘의 크기\n","\n","# hidden size of model\n","d_k = 64            # 임베딩 차원"]},{"cell_type":"markdown","metadata":{"id":"JPhjzSZ1mykL"},"source":["## 2. Embedding 및 Query, Key, Value Vector 구하기\n","\n","토큰화된 입력 데이터가 임베딩 과정을 거쳐 임베딩 공간의 `d_k` 차원으로 projection되어 임베딩 벡터가 되도록 임베딩 레이어를 정의합니다. PyTorch의 `nn.Embedding` 모듈을 이용할 수 있습니다.\n","\n","- 참고자료 (PyTorch Emedding): https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","이후, 임베딩 벡터를 각각 Query, Key, Value 벡터로 변환하는 Linear layer를 정의합니다. `d_k` 차원의 임베딩 벡터를 각각 `d_k` 차원을 가지는 3개의 벡터로 만들어야 합니다. PyTorch의 `nn.Linear` 모듈을 이용할 수 있습니다."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ltZATd6UnTz7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":8,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"2c67f9aa-ace4-448b-8aac-fe4cfbde5974"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of embedding: torch.Size([10, 20, 64])\n","Shape of Q, K, V: (torch.Size([10, 20, 64]), torch.Size([10, 20, 64]), torch.Size([10, 20, 64]))\n"]}],"source":["############################################################################\n","# Req 3-2: Embedding 및 Query, Key, Value Vector 구하기                     #\n","############################################################################\n","\n","################################################################################\n","# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n","# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# 3) d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","\n","# vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n","embedding_layer = nn.Embedding(vocab_size, d_k)\n","\n","# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# B: Batch, L: Max length of sequence\n","batch = torch.LongTensor(data) # (B, L)\n","batch_emb = embedding_layer(batch)  # (B, L, d_k)\n","print(f\"Shape of embedding: {batch_emb.shape}\")\n","\n","# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","w_q = nn.Linear(d_k, d_k)  # Q를 위한 Linear layer\n","w_k = nn.Linear(d_k, d_k)  # K를 위한 Linear layer\n","w_v = nn.Linear(d_k, d_k)  # V를 위한 Linear layer\n","\n","# 임베딩 벡터를 Q, K, V 벡터로 변환\n","q = w_q(batch_emb)  # (B, L, d_k)\n","k = w_k(batch_emb)  # (B, L, d_k)\n","v = w_v(batch_emb)  # (B, L, d_k)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"KIm8qW0Spp5-"},"source":["## 3. Scaled Dot-Product Self-Attention 구현하기\n","\n","![scaled-dot-product-atten](https://velog.velcdn.com/images/glad415/post/1645abbb-e260-4b82-ab1d-c8aa134ea8b7/image.png)\n","\n","각 head에서 진행하는 Q, K, V의 Self-attention을 구현하겠습니다.\n","\n","- 가장 먼저, 쿼리와 키(의 전치) 벡터를 곱한 뒤, 벡터 차원의 제곱근으로 나눕니다. 해당 과정은 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가를 연산합니다.\n","- 그 값에 softmax 함수를 취합니다.\n","- 그 값에 밸류 벡터를 곱하여 최종 attention matrix를 얻습니다."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"QwhSwgDOp3Hf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":7,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"3782299a-9328-409a-bf3f-2be5d2a6fa76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attn_scores: torch.Size([10, 20, 20])\n","Shape of attn_dists: torch.Size([10, 20, 20])\n","Shape of attn_values: torch.Size([10, 20, 64])\n"]}],"source":["############################################################################\n","# Req 3-3: Scaled Dot-Product Self-Attention 구현하기                        #\n","############################################################################\n","\n","################################################################################\n","# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","\n","# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# Output shape: (B, L, L)\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)  # (B, L, L)\n","print(f\"Shape of attn_scores: {attn_scores.shape}\")\n","\n","\n","# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# Output shape: (B, L, L)\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, L, L)\n","print(f\"Shape of attn_dists: {attn_dists.shape}\")\n","\n","# Value 벡터를 곱해 최종 attention value 계산\n","# Output shape: (B, L, d_k)\n","attn_values = torch.matmul(attn_dists, v)  # (B, L, d_k)\n","print(f\"Shape of attn_values: {attn_values.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"qT3oE93Pqxjl"},"source":["# Multi-Head Self-Attention 구현하기\n","\n","![multi-head-attn](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F2e70e601-e268-4b55-a8c9-11b3ff145d92%2Fimage.png)\n","\n","Single-head 코드를 바탕으로 multi-head self-attention을 구현하겠습니다. 전체적으론 아래 내용이 변경 혹은 추가되어야 합니다.\n","\n","- Q, K, V를 (임베딩 벡터 차원) x (head 개수) 차원으로 projection한 뒤 각 head 개수로 쪼개 사용합니다.\n","    - 임베딩 과정에서 입력 데이터를 임베딩 공간의 `d_model` 차원으로 projection합니다.\n","    - 해당 `d_model` 값은 어텐션 헤드의 개수(`n_heads`)로 나누어 떨어져야 합니다.\n","- 각 헤드의 연산을 한 뒤에는 각 헤드별로 가중치를 곱해 최종 attention value를 구합니다."]},{"cell_type":"markdown","metadata":{"id":"62tPF-_4wK2F"},"source":["아래는 multi-head self-attention 구현에 사용할 설정 값입니다."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"xPik0KzswK2F","executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":6,"user":{"displayName":"김아름","userId":"12348658311446229205"}}},"outputs":[],"source":["# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n","vocab_size = 100\n","\n","# hidden size of model\n","d_k = 64\n","\n","# hidden size of model\n","d_model = 512\n","\n","# number of attention heads\n","num_heads = 8"]},{"cell_type":"markdown","metadata":{"id":"sjMDtdJEsYuq"},"source":["## 4. Embedding 및 Query, Key, Value Vector 구하기\n","\n","- Embedding 시 주의사항\n","  - Single-Head와 마찬가지로 데이터를 임베딩 벡터로 전환해야 합니다. 단, 해당 임베딩 벡터를 나중엔 헤드의 수대로 쪼개야 하기 때문에, 이 부분을 유의합니다.\n","- Q, K, V 벡터 변환 시 주의사항\n","  - 이론적으로는 multi-head attention을 수행할 때, input을 각각 다른 head 개수만큼의 `w_q`, `w_k`, `w_v`로 linear transformation하여 각각 여러 번의 attention을 수행한 후 concat하고 linear transformation을 수행합니다.\n","  - 하지만 구현에서는 $W^Q$\\, $W^K$\\, $W^V$ 한 개씩만 사용합니다. 여러 헤드의 각 Q, K, V 벡터들을 한 번에 가진 긴 각 Q, K, V 벡터를 만드는 셈입니다.\n","  - 따라서 각 linear layers `w_q`, `w_k`, `w_v` 는 `d_model` 차원의 입력 임베딩 벡터를 `d_model` 차원의 Q, K, V로 만들어야 합니다.\n","  - 그리고 이렇게 긴 (여러 헤드의 값을 한 번에 가진) Q, K, V 벡터를 각 헤드만큼씩 쪼개야 합니다. 앞선 single-head attention 때와 마찬가지로, `d_k`는 각 헤드의 벡터의 차원으로, `d_model`이 헤드 개수 만큼씩 쪼개져 들어갑니다. 즉, 각 Q, K, V 벡터를 `d_model` → (num_heads, d_k)` 형태로 변경하는 작업을 합니다."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"a3YRujo-sTeo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":6,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"6b99ed21-3a39-4f97-9daf-3ba252a65915"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of embedding: torch.Size([10, 20, 512])\n","Shape of Q, K, V: (torch.Size([10, 20, 512]), torch.Size([10, 20, 512]), torch.Size([10, 20, 512]))\n","Shape of Q, K, V: (torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]))\n"]}],"source":["############################################################################\n","# Req 3-4: Embedding 및 Query, Key, Value Vector 구하기                      #\n","############################################################################\n","\n","################################################################################\n","# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n","# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# 3) d_model 차원의 Q, K, V 벡터를 만들 레이어 정의\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","# 5) d_k 차원 계산\n","# 6) Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n","# 7) Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n","\n","# vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n","embedding_layer = nn.Embedding(vocab_size, d_model)\n","\n","# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# B: Batch, L: Max length of sequence\n","batch = torch.LongTensor(data) # (B, L)\n","batch_size = batch.shape[0]\n","batch_emb = embedding_layer(batch)  # (B, L, d_model)\n","print(f\"Shape of embedding: {batch_emb.shape}\")\n","\n","# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","w_q = nn.Linear(d_model, d_model)  # Q를 위한 Linear layer\n","w_k = nn.Linear(d_model, d_model)  # K를 위한 Linear layer\n","w_v = nn.Linear(d_model, d_model)  # V를 위한 Linear layer\n","\n","# 임베딩 벡터를 Q, K, V 벡터로 변환\n","q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# d_k 차원 계산\n","d_k = d_model // num_heads #\"\"\"Write your code\"\"\"\n","\n","# Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","# Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"ZACYsG4is6Pz"},"source":["## 5. Scaled Dot-Product Self-Attention 구현하기\n","\n","해당 부분은 각 head에서 진행되는 attention 연산으로, 코드는 single-head와 동일합니다. 그러나 해당 연산이 이루어지는 matrix의 형태가 다릅니다."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"dNFEEnM9s5xt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724997556332,"user_tz":-540,"elapsed":5,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"2c11360c-fca4-4c87-c8fc-0cc616f284a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attn_scores: torch.Size([10, 8, 20, 20])\n","Shape of attn_dists: torch.Size([10, 8, 20, 20])\n","Shape of attn_values: torch.Size([10, 8, 20, 64])\n"]}],"source":["############################################################################\n","# Req 3-5: Scaled Dot-Product Self-Attention 구현하기                        #\n","############################################################################\n","\n","################################################################################\n","# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","\n","# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# Output shape - (B, num_heads, L, L)\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (d_k ** 0.5)  # (B, L, L)\n","print(f\"Shape of attn_scores: {attn_scores.shape}\")\n","\n","# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# Output shape: (B, num_heads, L, L)\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","print(f\"Shape of attn_dists: {attn_dists.shape}\")\n","\n","# Value 벡터를 곱해 최종 attention value 계산\n","# Output shape: (B, num_heads, L, d_k)\n","attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","print(f\"Shape of attn_values: {attn_values.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"ciMG2MXsqWhu"},"source":["## 6. Attention heads의 결과물 병합하기\n","\n","각 attention head의 결과물을 concatenate해 병합하고, head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정합니다.\n","이 linear projection은 서로 다른 의미로 focusing된 각 head의 self-attention 정보를 합치는 역할을 합니다."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"7wAhW2_Qu9Qn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724998283253,"user_tz":-540,"elapsed":2,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"31515e6a-3d13-4409-da98-88630e265f67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attention values from all heads: torch.Size([10, 20, 1280])\n","Shape of multi-head self-attention result: torch.Size([10, 20, 512])\n"]}],"source":["############################################################################\n","# Req 3-6: Attention heads의 결과물 병합하기                                  #\n","############################################################################\n","\n","################################################################################\n","# TODO: 각 attention head의 결과물을 병합하고 head 별 가중치로 projection하여        #\n","# 최종 attention 결과를 구함                                                     #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n","# 2) 각 head의 점수를 concatenate함\n","# 3) 각 head마다의 가중치를 부여할 linear layer 정의\n","# 4) 위 linear projection layer를 통과하여 최종 출력을 계산\n","\n","\n","# 임베딩 레이어 정의\n","embedding_layer = nn.Embedding(vocab_size, d_model)\n","batch_emb = embedding_layer(batch)  # (B, L, d_model)\n","\n","# 각 헤드의 결과를 위한 임의의 attn_values 생성 (B, num_heads, L, d_k)\n","attn_values = torch.randn(batch_size, num_heads, batch_emb.shape[1], d_k)  # 예시 (B, num_heads, L, d_k)\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n","attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","\n","# 2) 각 head의 점수를 concatenate함\n","# L과 d_k를 동적으로 가져옴\n","batch_size, L, num_heads, d_k = attn_values.shape  # 여기서 unpacking\n","attn_values = attn_values.contiguous().view(batch_size, L, -1)  # (B, L, num_heads * d_k)\n","print(f\"Shape of attention values from all heads: {attn_values.shape}\")\n","\n","# 3) 각 head마다의 가중치를 부여할 linear layer 정의\n","w_0 = nn.Linear(num_heads * d_k, d_model)  # (num_heads * d_k, d_model)\n","\n","# 4) 위 linear projection layer를 통과하여 최종 출력을 계산\n","outputs = w_0(attn_values)  # (B, L, d_model)\n","print(f\"Shape of multi-head self-attention result: {outputs.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"YVg9X9A9vilS"},"source":["# 7. Multi-Head Self-Attention의 모듈 클래스 구현\n","\n","앞선 <Req. 3-4> ~ <Req. 3-6>의 multi-head self-attention 구현 과정을 하나의 모듈 클래스로 만들어 처리를 손쉽게 구현하겠습니다.\n","\n","해당 클래스의 forward 연산에서는 임베딩 벡터를 입력으로 받아 multi-head self-attention 결과를 출력하도록 합니다."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"9ypP4_Sjv1aX","executionInfo":{"status":"ok","timestamp":1724998286986,"user_tz":-540,"elapsed":555,"user":{"displayName":"김아름","userId":"12348658311446229205"}}},"outputs":[],"source":["############################################################################\n","# Req 3-7: Multi-Head Self-Attention의 모듈 클래스 구현                       #\n","############################################################################\n","\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, d_model, d_k, num_heads):\n","        super(MultiheadAttention, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.num_heads = num_heads\n","\n","        ################################################################################\n","        # TODO: 조건에 맞게 모델의 레이어를 정의함                                          #\n","        ################################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # 1) 임베딩 벡터를 Q, K, V 벡터로 변환할 레이어 정의\n","        # 2) 각 헤드의 결과에 가중치를 부여할 linear layer 정의\n","\n","        # Q, K, V learnable matrices\n","        # 1) 임베딩 벡터를 Q, K, V 벡터로 변환할 레이어 정의\n","        self.w_q = nn.Linear(d_model, d_model)  # Q를 위한 Linear layer\n","        self.w_k = nn.Linear(d_model, d_model)  # K를 위한 Linear layer\n","        self.w_v = nn.Linear(d_model, d_model)  # V를 위한 Linear layer\n","\n","        # Linear projection for concatenated outputs\n","        self.w_o = nn.Linear(d_model, d_model)  # 최종 출력을 위한 Linear layer\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################\n","\n","    def self_attention(self, q, k, v):\n","        \"\"\"scaled-dot product attention\n","\n","        Args:\n","            q, k, v (torch.Tensor): Query, Key, Value vectors\n","        Returns:\n","            attn_values (torch.Tensor): Result of self-attention\n","        \"\"\"\n","        ################################################################################\n","        # TODO: Scaled dot-product self-attention 연산을 정의함                          #\n","        ################################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","        # 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","        # 3) Value 벡터를 곱해 최종 attention value 계산\n","\n","        # 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)  # (B, num_heads, L, L)\n","\n","        # 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1로 적용할 것.\n","        attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","        # 3) Value 벡터를 곱해 최종 attention value 계산\n","        attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################\n","\n","        return attn_values\n","\n","    def forward(self, batch_emb):\n","        batch_size = batch_emb.shape[0]\n","\n","        ################################################################################\n","        # TODO: Multi-head self-attention 과정을 정의함                                  #\n","        ################################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # 1) 입력 데이터를 Q, K, V 벡터로 변환\n","        # 2) 위 결과를 Head의 수로 분할함\n","        # 3) 각 head가 (L, d_k)의 matrix를 담당하도록 만듦\n","        # 4) Scaled dot-product self-attention 연산을 수행함\n","        # 5) 각 attention head의 결과물을 concatenate해 병합함\n","        # 6) head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정함\n","\n","        # 1) 입력 데이터를 Q, K, V 벡터로 변환\n","        q = self.w_q(batch_emb)  # (B, L, d_model)\n","        k = self.w_k(batch_emb)  # (B, L, d_model)\n","        v = self.w_v(batch_emb)  # (B, L, d_model)\n","\n","        # 2) 위 결과를 Head의 수로 분할함\n","        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (B, num_heads, L, d_k)\n","        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (B, num_heads, L, d_k)\n","        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","        # 3) 각 head가 (L, d_k)의 matrix를 담당하도록 만듦\n","        # (q, k, v)는 이미 (B, num_heads, L, d_k) 형태입니다.\n","\n","        # 4) Scaled dot-product self-attention 연산을 수행함\n","        attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n","\n","        # 5) 각 attention head의 결과물을 concatenate해 병합함\n","        attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # (B, L, d_model)\n","\n","        # 6) head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정함\n","        outputs = self.w_o(attn_values)  # (B, L, d_model)\n","\n","        return outputs\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################"]},{"cell_type":"markdown","metadata":{"id":"8sTGW9GOwK2H"},"source":["아래 코드로 결과를 테스트할 수 있습니다."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"dUj1DXQ7wFrz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724998290468,"user_tz":-540,"elapsed":499,"user":{"displayName":"김아름","userId":"12348658311446229205"}},"outputId":"d548d8a8-f17f-45e7-8017-690700d77818"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.1245,  0.0842, -0.0761,  ...,  0.1766,  0.1458,  0.2143],\n","         [ 0.0253,  0.1277,  0.0245,  ...,  0.0400, -0.0244, -0.1066],\n","         [-0.1119,  0.2120, -0.1211,  ...,  0.1040,  0.0908, -0.0629],\n","         ...,\n","         [-0.1123,  0.1798, -0.0751,  ...,  0.1715,  0.2028, -0.0963],\n","         [ 0.0614, -0.0875,  0.1310,  ..., -0.2403, -0.0384, -0.0449],\n","         [ 0.1872,  0.0861, -0.0043,  ..., -0.1211,  0.0783,  0.1106]],\n","\n","        [[ 0.0555,  0.4623,  0.1180,  ..., -0.0228,  0.1303, -0.1810],\n","         [-0.0681,  0.2951,  0.2360,  ...,  0.0986,  0.1621, -0.0319],\n","         [-0.0405,  0.4314, -0.0377,  ..., -0.0159,  0.0746, -0.1757],\n","         ...,\n","         [ 0.0230,  0.3590,  0.0697,  ...,  0.0754,  0.1753, -0.1957],\n","         [ 0.0245,  0.4841,  0.1009,  ...,  0.1807,  0.2318, -0.1447],\n","         [ 0.0416,  0.3988,  0.1030,  ...,  0.1449,  0.2803, -0.1220]],\n","\n","        [[-0.0413, -0.0686, -0.0503,  ...,  0.1010, -0.3001,  0.0131],\n","         [ 0.1307,  0.2541, -0.0031,  ..., -0.0716,  0.0737, -0.1181],\n","         [-0.0427,  0.3473, -0.0609,  ...,  0.1388,  0.2010, -0.0934],\n","         ...,\n","         [-0.0450,  0.2900, -0.0917,  ...,  0.1998,  0.2808, -0.0428],\n","         [ 0.0900,  0.3100,  0.0686,  ...,  0.1021,  0.0919, -0.1153],\n","         [ 0.1008,  0.3067,  0.0607,  ...,  0.1782,  0.1986, -0.0458]],\n","\n","        ...,\n","\n","        [[ 0.0425, -0.0590,  0.1282,  ..., -0.0797, -0.0299, -0.0664],\n","         [-0.0241,  0.0585, -0.0530,  ...,  0.1738, -0.2294, -0.0487],\n","         [ 0.1839, -0.3043, -0.1135,  ...,  0.0107, -0.0823, -0.1126],\n","         ...,\n","         [ 0.1645, -0.3045, -0.0446,  ..., -0.0560, -0.0846, -0.1105],\n","         [ 0.0764, -0.0522, -0.1097,  ..., -0.0975,  0.1111,  0.2364],\n","         [-0.1296,  0.0075, -0.1730,  ...,  0.0013, -0.1873,  0.2380]],\n","\n","        [[ 0.0325,  0.0393, -0.0612,  ...,  0.0964,  0.0647, -0.0613],\n","         [ 0.0072,  0.2050,  0.0139,  ...,  0.0963,  0.2671, -0.0070],\n","         [-0.1120,  0.1166,  0.0558,  ...,  0.1831,  0.0155,  0.0006],\n","         ...,\n","         [-0.1368,  0.0404,  0.0341,  ...,  0.1714,  0.0083, -0.0217],\n","         [-0.1010,  0.2990, -0.0678,  ..., -0.0945, -0.0359, -0.0882],\n","         [ 0.1938,  0.1395, -0.2021,  ...,  0.0199,  0.0114,  0.0039]],\n","\n","        [[-0.0168,  0.0114,  0.0308,  ...,  0.2241,  0.0382, -0.1923],\n","         [-0.0728,  0.2423, -0.1018,  ..., -0.2134, -0.0124, -0.1109],\n","         [ 0.0593,  0.2463, -0.0542,  ...,  0.0103,  0.1328, -0.0725],\n","         ...,\n","         [-0.0023,  0.2405,  0.0083,  ...,  0.1011,  0.1615, -0.0092],\n","         [ 0.0531,  0.2158, -0.0734,  ...,  0.0371, -0.0139,  0.1202],\n","         [-0.0278,  0.1995,  0.0116,  ...,  0.0313,  0.1416,  0.0297]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([10, 20, 512])\n"]}],"source":["multi_attn = MultiheadAttention(d_model, d_k, num_heads)\n","outputs = multi_attn(batch_emb)  # (B, L, d_model)\n","\n","print(outputs)\n","print(outputs.shape)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}