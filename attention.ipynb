{"cells":[{"cell_type":"markdown","metadata":{"id":"Ul3DDXN8_5e8"},"source":["## 과제 목표\n","1. Transformer의 multi-head self-attention의 이해 및 구현"]},{"cell_type":"markdown","metadata":{"id":"7qmQI_zM_5e9"},"source":["# 환경 세팅"]},{"cell_type":"markdown","metadata":{"id":"iogpAgcR_5e-"},"source":["본 실습에서 사용할 라이브러리를 import 하겠습니다."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"HQL3hEx55x0J","executionInfo":{"status":"ok","timestamp":1724983011195,"user_tz":-540,"elapsed":8733,"user":{"displayName":"jjj sss","userId":"09090373615646723049"}}},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","\n","import torch\n","import math"]},{"cell_type":"markdown","metadata":{"id":"4glEN8qd_5e-"},"source":["# 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"e1hQFJUy6dMl"},"source":["본 실습에서 사용할 가상의 데이터를 만들겠습니다. 총 10개 시퀀스의 데이터가 이미 토큰화가 진행되어 주어졌다 가정하겠습니다."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"01KXLeH16cUF","executionInfo":{"status":"ok","timestamp":1724983024156,"user_tz":-540,"elapsed":305,"user":{"displayName":"jjj sss","userId":"09090373615646723049"}}},"outputs":[],"source":["data = [\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n","  [60, 96, 51, 32, 90],\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n","  [75, 51],\n","  [66, 88, 98, 47],\n","  [21, 39, 10, 64, 21],\n","  [98],\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n","]"]},{"cell_type":"markdown","metadata":{"id":"OgPGGJA-_5e_"},"source":["## 1. Padding 함수 구현하기"]},{"cell_type":"markdown","metadata":{"id":"yL10MTzj6Vja"},"source":["어텐션 연산에 들어갈 데이터의 전처리를 진행하겠습니다. 본 실습의 전처리 과정에서는 데이터의 길이를 균일하게 맞출 padding 함수를 구현합니다.\n","\n","배치 내 데이터 중 최대 길이에 맞도록 다른 데이터의 마지막에 임의의 패딩 값을 패딩해 길이를 맞추는 함수입니다."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"U3i0HSml6wIR","executionInfo":{"status":"ok","timestamp":1724983043283,"user_tz":-540,"elapsed":2,"user":{"displayName":"jjj sss","userId":"09090373615646723049"}}},"outputs":[],"source":["############################################################################\n","# Req 3-1: Padding 함수 구현하기                                             #\n","############################################################################\n","\n","def padding(data, pad_value=0):\n","    \"\"\"\n","    Args:\n","        data (list[list[int]]): data sequence\n","        pad_value (int): value to pad\n","    Returns:\n","        data (list[list[int]]): padded data sequence\n","        max_len (int): maximum length of intput data\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 배치 내 데이터 중 최대 길이에 맞추어 다른 데이터의 뒷부분에 패딩 값을 붙여        #\n","    # 길이를 동일하게 맞추어 반환함                                                     #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # 1) 가장 긴 데이터의 길이 추출\n","    max_len = max(len(seq) for seq in data)\n","\n","    # 2) 모든 데이터를 돌면서 1에서 구한 길이에 맞게 뒷부분을 pad_value로 채우기\n","    data = [seq + [pad_value] * (max_len - len(seq)) for seq in data]\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################\n","\n","    return data, max_len"]},{"cell_type":"markdown","metadata":{"id":"JpRsBIa0_5fA"},"source":["아래 코드로 패딩 결과를 확인할 수 있습니다."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mO6N-pYJ7s9O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724983046346,"user_tz":-540,"elapsed":331,"user":{"displayName":"jjj sss","userId":"09090373615646723049"}},"outputId":"bd57ab02-e299-4ff2-c175-d267ccbc2221"},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 20\n","[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0]\n","[60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10]\n","[70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0]\n","[20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]\n"]}],"source":["pad_value = 0 # 패딩(Padding)은 제로(zero) 패딩으로, 빈 공간을 0으로 채움.\n","data, max_len = padding(data)\n","print(f\"Maximum sequence length: {max_len}\")\n","for d in data:\n","    print(d)"]},{"cell_type":"markdown","metadata":{"id":"6SapaNXN6N4Z"},"source":["# Single-head Self-Attention 구현하기"]},{"cell_type":"markdown","metadata":{"id":"zsxyFEez_5fA"},"source":["Multi-head attention을 구현하기 앞서, 먼저 single-head attention을 구현해보겠습니다. 아래는 single-head self-attention을 위한 설정 값입니다."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Pd3xtS7R_5fA","executionInfo":{"status":"ok","timestamp":1724983049997,"user_tz":-540,"elapsed":313,"user":{"displayName":"jjj sss","userId":"09090373615646723049"}}},"outputs":[],"source":["# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n","vocab_size = 100\n","\n","# hidden size of model\n","d_k = 64"]},{"cell_type":"markdown","metadata":{"id":"JPhjzSZ1mykL"},"source":["## 2. Embedding 및 Query, Key, Value Vector 구하기\n","\n","토큰화된 입력 데이터가 임베딩 과정을 거쳐 임베딩 공간의 `d_k` 차원으로 projection되어 임베딩 벡터가 되도록 임베딩 레이어를 정의합니다. PyTorch의 `nn.Embedding` 모듈을 이용할 수 있습니다.\n","\n","- 참고자료 (PyTorch Emedding): https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","\n","이후, 임베딩 벡터를 각각 Query, Key, Value 벡터로 변환하는 Linear layer를 정의합니다. `d_k` 차원의 임베딩 벡터를 각각 `d_k` 차원을 가지는 3개의 벡터로 만들어야 합니다. PyTorch의 `nn.Linear` 모듈을 이용할 수 있습니다."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312,"status":"ok","timestamp":1724983433266,"user":{"displayName":"jjj sss","userId":"09090373615646723049"},"user_tz":-540},"id":"ltZATd6UnTz7","outputId":"042473b2-f91e-4ef3-ccf2-544abe295d34"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of embedding: torch.Size([10, 20, 64])\n","Shape of Q, K, V: (torch.Size([10, 20, 64]), torch.Size([10, 20, 64]), torch.Size([10, 20, 64]))\n"]}],"source":["############################################################################\n","# Req 3-2: Embedding 및 Query, Key, Value Vector 구하기                     #\n","############################################################################\n","\n","################################################################################\n","# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n","# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# 3) d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","\n","# vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n","embedding_layer = nn.Embedding(vocab_size, d_k)\n","\n","# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# B: Batch, L: Max length of sequence\n","batch = torch.LongTensor(data) # (B, L)\n","batch_emb = embedding_layer(batch)  # (B, L, d_k)\n","print(f\"Shape of embedding: {batch_emb.shape}\")\n","\n","# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","w_q = nn.Linear(d_k, d_k)\n","w_k = nn.Linear(d_k, d_k)\n","w_v = nn.Linear(d_k, d_k)\n","\n","# 임베딩 벡터를 Q, K, V 벡터로 변환\n","q = w_q(batch_emb)  # (B, L, d_k)\n","k = w_k(batch_emb)  # (B, L, d_k)\n","v = w_v(batch_emb)  # (B, L, d_k)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"KIm8qW0Spp5-"},"source":["## 3. Scaled Dot-Product Self-Attention 구현하기\n","\n","![scaled-dot-product-atten](https://velog.velcdn.com/images/glad415/post/1645abbb-e260-4b82-ab1d-c8aa134ea8b7/image.png)\n","\n","각 head에서 진행하는 Q, K, V의 Self-attention을 구현하겠습니다.\n","\n","- 가장 먼저, 쿼리와 키(의 전치) 벡터를 곱한 뒤, 벡터 차원의 제곱근으로 나눕니다. 해당 과정은 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가를 연산합니다.\n","- 그 값에 softmax 함수를 취합니다.\n","- 그 값에 밸류 벡터를 곱하여 최종 attention matrix를 얻습니다."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1724983642938,"user":{"displayName":"jjj sss","userId":"09090373615646723049"},"user_tz":-540},"id":"QwhSwgDOp3Hf","outputId":"8ea22bf4-ba90-4ef7-eda5-64d675ab296d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attn_scores: torch.Size([10, 20, 20])\n","Shape of attn_dists: torch.Size([10, 20, 20])\n","Shape of attn_values: torch.Size([10, 20, 64])\n"]}],"source":["############################################################################\n","# Req 3-3: Scaled Dot-Product Self-Attention 구현하기                        #\n","############################################################################\n","\n","################################################################################\n","# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# Output shape: (B, L, L)\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","print(f\"Shape of attn_scores: {attn_scores.shape}\")\n","\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# Output shape: (B, L, L)\n","attn_dists = F.softmax(attn_scores, dim = -1)\n","print(f\"Shape of attn_dists: {attn_dists.shape}\")\n","\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","# Output shape: (B, L, d_k)\n","attn_values = torch.matmul(attn_dists, v)\n","print(f\"Shape of attn_values: {attn_values.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"qT3oE93Pqxjl"},"source":["# Multi-Head Self-Attention 구현하기\n","\n","![multi-head-attn](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F2e70e601-e268-4b55-a8c9-11b3ff145d92%2Fimage.png)\n","\n","Single-head 코드를 바탕으로 multi-head self-attention을 구현하겠습니다. 전체적으론 아래 내용이 변경 혹은 추가되어야 합니다.\n","\n","- Q, K, V를 (임베딩 벡터 차원) x (head 개수) 차원으로 projection한 뒤 각 head 개수로 쪼개 사용합니다.\n","    - 임베딩 과정에서 입력 데이터를 임베딩 공간의 `d_model` 차원으로 projection합니다.\n","    - 해당 `d_model` 값은 어텐션 헤드의 개수(`n_heads`)로 나누어 떨어져야 합니다.\n","- 각 헤드의 연산을 한 뒤에는 각 헤드별로 가중치를 곱해 최종 attention value를 구합니다."]},{"cell_type":"markdown","metadata":{"id":"PEgNsjP5_5fB"},"source":["아래는 multi-head self-attention 구현에 사용할 설정 값입니다."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"IeMfdWuN_5fB","executionInfo":{"status":"ok","timestamp":1724983908664,"user_tz":-540,"elapsed":329,"user":{"displayName":"jjj sss","userId":"09090373615646723049"}}},"outputs":[],"source":["# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n","vocab_size = 100\n","\n","# hidden size of model\n","d_k = 64\n","\n","# hidden size of model\n","d_model = 512\n","\n","# number of attention heads\n","num_heads = 8"]},{"cell_type":"markdown","metadata":{"id":"sjMDtdJEsYuq"},"source":["## 4. Embedding 및 Query, Key, Value Vector 구하기\n","\n","- Embedding 시 주의사항\n","  - Single-Head와 마찬가지로 데이터를 임베딩 벡터로 전환해야 합니다. 단, 해당 임베딩 벡터를 나중엔 헤드의 수대로 쪼개야 하기 때문에, 이 부분을 유의합니다.\n","- Q, K, V 벡터 변환 시 주의사항\n","  - 이론적으로는 multi-head attention을 수행할 때, input을 각각 다른 head 개수만큼의 `w_q`, `w_k`, `w_v`로 linear transformation하여 각각 여러 번의 attention을 수행한 후 concat하고 linear transformation을 수행합니다.\n","  - 하지만 구현에서는 $W^Q$\\, $W^K$\\, $W^V$ 한 개씩만 사용합니다. 여러 헤드의 각 Q, K, V 벡터들을 한 번에 가진 긴 각 Q, K, V 벡터를 만드는 셈입니다.\n","  - 따라서 각 linear layers `w_q`, `w_k`, `w_v` 는 `d_model` 차원의 입력 임베딩 벡터를 `d_model` 차원의 Q, K, V로 만들어야 합니다.\n","  - 그리고 이렇게 긴 (여러 헤드의 값을 한 번에 가진) Q, K, V 벡터를 각 헤드만큼씩 쪼개야 합니다. 앞선 single-head attention 때와 마찬가지로, `d_k`는 각 헤드의 벡터의 차원으로, `d_model`이 헤드 개수 만큼씩 쪼개져 들어갑니다. 즉, 각 Q, K, V 벡터를 `d_model` → (num_heads, d_k)` 형태로 변경하는 작업을 합니다."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1724984102599,"user":{"displayName":"jjj sss","userId":"09090373615646723049"},"user_tz":-540},"id":"a3YRujo-sTeo","outputId":"2f7c8c34-fce3-43ee-a60e-d8979740d98b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of embedding: torch.Size([10, 20, 512])\n","Shape of Q, K, V: (torch.Size([10, 20, 512]), torch.Size([10, 20, 512]), torch.Size([10, 20, 512]))\n","Shape of Q, K, V: (torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]))\n"]}],"source":["############################################################################\n","# Req 3-4: Embedding 및 Query, Key, Value Vector 구하기                      #\n","############################################################################\n","\n","################################################################################\n","# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n","# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# 3) d_model 차원의 Q, K, V 벡터를 만들 레이어 정의\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","# 5) d_k 차원 계산\n","# 6) Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n","# 7) Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n","\n","# vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n","embedding_layer = nn.Embedding(vocab_size, d_model)\n","\n","# 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# B: Batch, L: Max length of sequence\n","batch = torch.LongTensor(data) # (B, L)\n","batch_size = batch.shape[0]\n","batch_emb = embedding_layer(batch)  # (B, L, d_model)\n","print(f\"Shape of embedding: {batch_emb.shape}\")\n","\n","# d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","w_q = nn.Linear(d_model, d_model)\n","w_k = nn.Linear(d_model, d_model)\n","w_v = nn.Linear(d_model, d_model)\n","\n","# 임베딩 벡터를 Q, K, V 벡터로 변환\n","q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# d_k 차원 계산\n","d_k = d_model // num_heads #\"\"\"Write your code\"\"\"\n","\n","# Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","# Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"ZACYsG4is6Pz"},"source":["## 5. Scaled Dot-Product Self-Attention 구현하기\n","\n","해당 부분은 각 head에서 진행되는 attention 연산으로, 코드는 single-head와 동일합니다. 그러나 해당 연산이 이루어지는 matrix의 형태가 다릅니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNFEEnM9s5xt"},"outputs":[],"source":["############################################################################\n","# Req 3-5: Scaled Dot-Product Self-Attention 구현하기                        #\n","############################################################################\n","\n","################################################################################\n","# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","\n","# Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# Output shape - (B, num_heads, L, L)\n","attn_scores = \"\"\"Write your code\"\"\"\n","print(f\"Shape of attn_scores: {attn_scores.shape}\")\n","\n","# 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# Output shape: (B, num_heads, L, L)\n","attn_dists = \"\"\"Write your code\"\"\"\n","print(f\"Shape of attn_dists: {attn_dists.shape}\")\n","\n","# Value 벡터를 곱해 최종 attention value 계산\n","# Output shape: (B, num_heads, L, d_k)\n","attn_values = \"\"\"Write your code\"\"\"\n","print(f\"Shape of attn_values: {attn_values.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"ciMG2MXsqWhu"},"source":["## 6. Attention heads의 결과물 병합하기\n","\n","각 attention head의 결과물을 concatenate해 병합하고, head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정합니다.\n","이 linear projection은 서로 다른 의미로 focusing된 각 head의 self-attention 정보를 합치는 역할을 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wAhW2_Qu9Qn"},"outputs":[],"source":["############################################################################\n","# Req 3-6: Attention heads의 결과물 병합하기                                  #\n","############################################################################\n","\n","################################################################################\n","# TODO: 각 attention head의 결과물을 병합하고 head 별 가중치로 projection하여        #\n","# 최종 attention 결과를 구함                                                     #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n","# 2) 각 head의 점수를 concatenate함\n","# 3) 각 head마다의 가중치를 부여할 linear layer 정의\n","# 4) 위 linear projection layer를 통과하여 최종 출력을 계산\n","\n","\n","# Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n","attn_values = \"\"\"Write your code\"\"\" # (B, L, num_heads, d_k)\n","\n","# 각 head의 점수를 concatenate함\n","attn_values = \"\"\"Write your code\"\"\" # (B, L, d_model)\n","print(f\"Shape of attention values from all heads: {attn_values.shape}\")\n","\n","# 각 head마다의 가중치를 부여할 linear layer 정의\n","w_0 = \"\"\"Write your code\"\"\"\n","\n","# 위 linear projection layer를 통과하여 최종 출력을 계산\n","outputs = \"\"\"Write your code\"\"\" # (B, L, d_model) -> (B, L, d_model)\n","print(f\"Shape of multi-head self-attention result: {outputs.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"YVg9X9A9vilS"},"source":["# 7. Multi-Head Self-Attention의 모듈 클래스 구현\n","\n","앞선 <Req. 3-4> ~ <Req. 3-6>의 multi-head self-attention 구현 과정을 하나의 모듈 클래스로 만들어 처리를 손쉽게 구현하겠습니다.\n","\n","해당 클래스의 forward 연산에서는 임베딩 벡터를 입력으로 받아 multi-head self-attention 결과를 출력하도록 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ypP4_Sjv1aX"},"outputs":[],"source":["############################################################################\n","# Req 3-7: Multi-Head Self-Attention의 모듈 클래스 구현                       #\n","############################################################################\n","\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, d_model, d_k, num_heads):\n","        super(MultiheadAttention, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.num_heads = num_heads\n","\n","        ################################################################################\n","        # TODO: 조건에 맞게 모델의 레이어를 정의함                                          #\n","        ################################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # 1) 임베딩 벡터를 Q, K, V 벡터로 변환할 레이어 정의\n","        # 2) 각 헤드의 결과에 가중치를 부여할 linear layer 정의\n","\n","        # Q, K, V learnable matrices\n","        \"\"\"Write your code\"\"\"\n","\n","        # Linear projection for concatenated outputs\n","        \"\"\"Write your code\"\"\"\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################\n","\n","    def self_attention(self, q, k, v):\n","        \"\"\"scaled-dot product attention\n","\n","        Args:\n","            q, k, v (torch.Tensor): Query, Key, Value vectors\n","        Returns:\n","            attn_values (torch.Tensor): Result of self-attention\n","        \"\"\"\n","        ################################################################################\n","        # TODO: Scaled dot-product self-attention 연산을 정의함                          #\n","        ################################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","        # 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","        # 3) Value 벡터를 곱해 최종 attention value 계산\n","\n","        \"\"\"Write your code\"\"\"\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################\n","\n","        return attn_values\n","\n","    def forward(self, batch_emb):\n","        batch_size = batch_emb.shape[0]\n","\n","        ################################################################################\n","        # TODO: Multi-head self-attention 과정을 정의함                                  #\n","        ################################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        # 1) 입력 데이터를 Q, K, V 벡터로 변환\n","        # 2) 위 결과를 Head의 수로 분할함\n","        # 3) 각 head가 (L, d_k)의 matrix를 담당하도록 만듦\n","        # 4) Scaled dot-product self-attention 연산을 수행함\n","        # 5) 각 attention head의 결과물을 concatenate해 병합함\n","        # 6) head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정함\n","\n","        \"\"\"Write your code\"\"\"\n","\n","        return outputs\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################"]},{"cell_type":"markdown","metadata":{"id":"QandOX2r_5fC"},"source":["아래 코드로 결과를 테스트할 수 있습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUj1DXQ7wFrz"},"outputs":[],"source":["multi_attn = MultiheadAttention(d_model, d_k, num_heads)\n","outputs = multi_attn(batch_emb)  # (B, L, d_model)\n","\n","print(outputs)\n","print(outputs.shape)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}