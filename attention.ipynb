{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPF+AQevFq3E9N9sKuZa2Z7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":17,"metadata":{"id":"m5_oViuZmDYt","executionInfo":{"status":"ok","timestamp":1724993824647,"user_tz":-540,"elapsed":321,"user":{"displayName":"신혜민","userId":"17175468705928178146"}}},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","\n","import torch\n","import math"]},{"cell_type":"code","source":["data = [\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n","  [60, 96, 51, 32, 90],\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n","  [75, 51],\n","  [66, 88, 98, 47],\n","  [21, 39, 10, 64, 21],\n","  [98],\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n","]"],"metadata":{"id":"-FmHDYM_mFDB","executionInfo":{"status":"ok","timestamp":1724993827513,"user_tz":-540,"elapsed":335,"user":{"displayName":"신혜민","userId":"17175468705928178146"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["############################################################################\n","# Req 3-1: Padding 함수 구현하기                                             #\n","############################################################################\n","\n","def padding(data, pad_value=0):\n","    \"\"\"\n","    Args:\n","        data (list[list[int]]): data sequence\n","        pad_value (int): value to pad\n","    Returns:\n","        data (list[list[int]]): padded data sequence\n","        max_len (int): maximum length of intput data\n","    \"\"\"\n","    ################################################################################\n","    # TODO: 배치 내 데이터 중 최대 길이에 맞추어 다른 데이터의 뒷부분에 패딩 값을 붙여        #\n","    # 길이를 동일하게 맞추어 반환함                                                     #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    # 1) 가장 긴 데이터의 길이 추출\n","    # 2) 모든 데이터를 돌면서 1에서 구한 길이에 맞게 뒷부분을 pad_value로 채우기\n","\n","    # 1) 가장 긴 데이터의 길이 추출\n","    max_len = max(len(seq) for seq in data)\n","\n","    # 2) 모든 데이터를 돌면서 1에서 구한 길이에 맞게 뒷부분을 pad_value로 채우기\n","    data2 = []\n","\n","    for seq in data:\n","        # 현재 시퀀스의 길이가 max_len보다 짧으면 pad_value로 패딩을 추가\n","        if len(seq) < max_len:\n","            seq = seq + [pad_value] * (max_len - len(seq))\n","        data2.append(seq)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ################################################################################\n","    #                                 END OF YOUR CODE                             #\n","    ################################################################################\n","\n","    return data2, max_len"],"metadata":{"id":"VqQLFm4zmUJK","executionInfo":{"status":"ok","timestamp":1724993831055,"user_tz":-540,"elapsed":333,"user":{"displayName":"신혜민","userId":"17175468705928178146"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["pad_value = 0 # 패딩(Padding)은 제로(zero) 패딩으로, 빈 공간을 0으로 채움.\n","data, max_len = padding(data)\n","print(f\"Maximum sequence length: {max_len}\")\n","for d in data:\n","    print(d)"],"metadata":{"id":"M8QIutNDnEAC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724993834172,"user_tz":-540,"elapsed":331,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"4d4e6e05-2392-4776-8b0f-01797bce682c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 20\n","[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0]\n","[60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10]\n","[70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0]\n","[20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","source":["# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n","vocab_size = 100\n","\n","# hidden size of model\n","d_k = 64"],"metadata":{"id":"bnuWIt6ppW7F","executionInfo":{"status":"ok","timestamp":1724993866714,"user_tz":-540,"elapsed":344,"user":{"displayName":"신혜민","userId":"17175468705928178146"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["############################################################################\n","# Req 3-2: Embedding 및 Query, Key, Value Vector 구하기                     #\n","############################################################################\n","\n","################################################################################\n","# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n","# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# 1) vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩을 할 레이어 정의\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","# 3) d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","\n","embedding_dim = d_k  # 임베딩 벡터의 차원, Q, K, V와 동일하게 설정\n","\n","# 1) vocab_size 개수의 데이터 각각을 d_k 차원으로 표현할 임베딩 레이어 정의\n","embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_emb = embedding(batch)  # (B, L, d_k)\n","print(f\"Shape of embedding: {batch_emb.shape}\")\n","\n","# 3) d_k 차원의 Q, K, V 벡터를 만들 레이어 정의\n","w_q = nn.Linear(embedding_dim, d_k)\n","w_k = nn.Linear(embedding_dim, d_k)\n","w_v = nn.Linear(embedding_dim, d_k)\n","\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","q = w_q(batch_emb)  # (B, L, d_k)\n","k = w_k(batch_emb)  # (B, L, d_k)\n","v = w_v(batch_emb)  # (B, L, d_k)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzIV6X71pY3i","executionInfo":{"status":"ok","timestamp":1724994018559,"user_tz":-540,"elapsed":338,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"cd3b4ca1-9be8-46f5-99ca-51dd2f70d0cf"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of embedding: torch.Size([10, 20, 64])\n","Shape of Q, K, V: (torch.Size([10, 20, 64]), torch.Size([10, 20, 64]), torch.Size([10, 20, 64]))\n"]}]},{"cell_type":"code","source":["############################################################################\n","# Req 3-3: Scaled Dot-Product Self-Attention 구현하기                        #\n","############################################################################\n","\n","################################################################################\n","# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# Output shape: (B, L, L)\n","attn_scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n","print(f\"Shape of attn_scores: {attn_scores.shape}\")\n","\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# Output shape: (B, L, L)\n","attn_dists = F.softmax(attn_scores, dim=-1)\n","print(f\"Shape of attn_dists: {attn_dists.shape}\")\n","\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","# Output shape: (B, L, d_k)\n","attn_values = torch.bmm(attn_dists, v)\n","print(f\"Shape of attn_values: {attn_values.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqxjLGjCp-Tw","executionInfo":{"status":"ok","timestamp":1724994083947,"user_tz":-540,"elapsed":854,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"82aa39ec-54c0-4374-b8cc-2e9e71974896"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attn_scores: torch.Size([10, 20, 20])\n","Shape of attn_dists: torch.Size([10, 20, 20])\n","Shape of attn_values: torch.Size([10, 20, 64])\n"]}]},{"cell_type":"code","source":["# 한 데이터의 최대 길이(vocab_size)는 100이라 가정.\n","vocab_size = 100\n","\n","# hidden size of model\n","d_k = 64\n","\n","# hidden size of model\n","d_model = 512\n","\n","# number of attention heads\n","num_heads = 8"],"metadata":{"id":"UYe-zbXxqOSD","executionInfo":{"status":"ok","timestamp":1724994093990,"user_tz":-540,"elapsed":1,"user":{"displayName":"신혜민","userId":"17175468705928178146"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["############################################################################\n","# Req 3-4: Embedding 및 Query, Key, Value Vector 구하기                      #\n","############################################################################\n","\n","################################################################################\n","# TODO: Embedding 레이어를 정의하고, 입력 데이터를 임베딩화함                         #\n","# 또한 Q, K, V를 위한 레이어를 정의하고, 임베딩 벡터를 각각 Q, K, V 벡터로 변환함       #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# 1) vocab_size 개수의 데이터 각각을 d_model 차원으로 표현할 임베딩을 할 레이어 정의\n","embedding = nn.Embedding(vocab_size, d_model)\n","\n","# 2) 임베딩 레이어를 사용하여 입력 데이터를 임베딩 벡터로 projection\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_size = batch.shape[0]\n","batch_emb = embedding(batch)  # (B, L, d_model)\n","print(f\"Shape of embedding: {batch_emb.shape}\")\n","\n","# 3) d_model 차원의 Q, K, V 벡터를 만들 레이어 정의\n","w_q = nn.Linear(d_model, d_model)\n","w_k = nn.Linear(d_model, d_model)\n","w_v = nn.Linear(d_model, d_model)\n","\n","# 4) 임베딩 벡터를 Q, K, V 벡터로 변환\n","q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# 5) d_k 차원 계산\n","# 이미 d_k = d_model // num_heads로 설정되어 있음\n","\n","# 6) Q, K, V 벡터를 각 헤드만큼 쪼갬 (각 헤드의 각 벡터 차원은 d_k)\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","# 7) Self-attention 연산을 위하여 각 헤드가 (L, d_k) 행렬을 갖도록 축을 transpose\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","print(f\"Shape of Q, K, V: {q.shape, k.shape, v.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbbex5j5qQiJ","executionInfo":{"status":"ok","timestamp":1724994153180,"user_tz":-540,"elapsed":317,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"c12a8e21-b5de-482e-f488-5dd10f127495"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of embedding: torch.Size([10, 20, 512])\n","Shape of Q, K, V: (torch.Size([10, 20, 512]), torch.Size([10, 20, 512]), torch.Size([10, 20, 512]))\n","Shape of Q, K, V: (torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]), torch.Size([10, 8, 20, 64]))\n"]}]},{"cell_type":"code","source":["############################################################################\n","# Req 3-5: Scaled Dot-Product Self-Attention 구현하기                        #\n","############################################################################\n","\n","################################################################################\n","# TODO: 수식에 맞추어 scaled dot-product self-attention을 구현함                   #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","# Output shape - (B, num_heads, L, L)\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n","print(f\"Shape of attn_scores: {attn_scores.shape}\")\n","\n","# 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","# Output shape: (B, num_heads, L, L)\n","attn_dists = F.softmax(attn_scores, dim=-1)\n","print(f\"Shape of attn_dists: {attn_dists.shape}\")\n","\n","# 3) Value 벡터를 곱해 최종 attention value 계산\n","# Output shape: (B, num_heads, L, d_k)\n","attn_values = torch.matmul(attn_dists, v)\n","print(f\"Shape of attn_values: {attn_values.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_eO2blMqfCZ","executionInfo":{"status":"ok","timestamp":1724994179750,"user_tz":-540,"elapsed":342,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"41ad613a-d617-4aed-9df8-d40d488ab21d"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attn_scores: torch.Size([10, 8, 20, 20])\n","Shape of attn_dists: torch.Size([10, 8, 20, 20])\n","Shape of attn_values: torch.Size([10, 8, 20, 64])\n"]}]},{"cell_type":"code","source":["############################################################################\n","# Req 3-6: Attention heads의 결과물 병합하기                                  #\n","############################################################################\n","\n","################################################################################\n","# TODO: 각 attention head의 결과물을 병합하고 head 별 가중치로 projection하여        #\n","# 최종 attention 결과를 구함                                                     #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# 1) Scaled dot-product 의 결과(attn_values)를 (B, L, num_heads, d_k) 형태로 축 변경\n","attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","print(f\"Shape of attn_values after transpose: {attn_values.shape}\")\n","\n","# 2) 각 head의 점수를 concatenate함\n","attn_values = attn_values.reshape(batch_size, -1, d_model)  # (B, L, d_model)\n","print(f\"Shape of attention values from all heads: {attn_values.shape}\")\n","\n","# 3) 각 head마다의 가중치를 부여할 linear layer 정의\n","w_0 = nn.Linear(d_model, d_model)\n","\n","# 4) 위 linear projection layer를 통과하여 최종 출력을 계산\n","outputs = w_0(attn_values)  # (B, L, d_model)\n","print(f\"Shape of multi-head self-attention result: {outputs.shape}\")\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             #\n","################################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6IIcQorqloK","executionInfo":{"status":"ok","timestamp":1724994206865,"user_tz":-540,"elapsed":348,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"ffed362b-0ffb-453c-bc2e-70726c62d0c6"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of attn_values after transpose: torch.Size([10, 20, 8, 64])\n","Shape of attention values from all heads: torch.Size([10, 20, 512])\n","Shape of multi-head self-attention result: torch.Size([10, 20, 512])\n"]}]},{"cell_type":"code","source":["############################################################################\n","# Req 3-7: Multi-Head Self-Attention의 모듈 클래스 구현                       #\n","############################################################################\n","\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, d_model, d_k, num_heads):\n","        super(MultiheadAttention, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_k = d_k\n","        self.num_heads = num_heads\n","\n","        # 1) 임베딩 벡터를 Q, K, V 벡터로 변환할 레이어 정의\n","        # Each of these layers maps d_model to d_k\n","        self.w_q = nn.Linear(d_model, d_k * num_heads)\n","        self.w_k = nn.Linear(d_model, d_k * num_heads)\n","        self.w_v = nn.Linear(d_model, d_k * num_heads)\n","\n","        # 2) 각 헤드의 결과에 가중치를 부여할 linear layer 정의\n","        self.w_o = nn.Linear(d_k * num_heads, d_model)\n","\n","    def self_attention(self, q, k, v):\n","        \"\"\"scaled-dot product attention\n","\n","        Args:\n","            q, k, v (torch.Tensor): Query, Key, Value vectors\n","        Returns:\n","            attn_values (torch.Tensor): Result of self-attention\n","        \"\"\"\n","        # 1) Query 벡터와 Key 벡터의 전치를 곱하고, 벡터 차원의 제곱근으로 나눔 (=(Q x K^T) / sqrt(d_k))\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n","\n","        # 2) 위 값에 softmax를 취함. row-wise이기 때문에 dim은 -1 로 적용할 것.\n","        attn_dists = F.softmax(attn_scores, dim=-1)\n","\n","        # 3) Value 벡터를 곱해 최종 attention value 계산\n","        attn_values = torch.matmul(attn_dists, v)\n","\n","        return attn_values\n","\n","    def forward(self, batch_emb):\n","        batch_size = batch_emb.shape[0]\n","        seq_len = batch_emb.shape[1]\n","\n","        # 1) 입력 데이터를 Q, K, V 벡터로 변환\n","        q = self.w_q(batch_emb)  # (B, L, d_k * num_heads)\n","        k = self.w_k(batch_emb)  # (B, L, d_k * num_heads)\n","        v = self.w_v(batch_emb)  # (B, L, d_k * num_heads)\n","\n","        # 2) 위 결과를 Head의 수로 분할함\n","        q = q.view(batch_size, seq_len, self.num_heads, self.d_k)  # (B, L, num_heads, d_k)\n","        k = k.view(batch_size, seq_len, self.num_heads, self.d_k)  # (B, L, num_heads, d_k)\n","        v = v.view(batch_size, seq_len, self.num_heads, self.d_k)  # (B, L, num_heads, d_k)\n","\n","        # 3) 각 head가 (L, d_k)의 matrix를 담당하도록 만듦\n","        q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","        k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","        v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","        # 4) Scaled dot-product self-attention 연산을 수행함\n","        attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n","\n","        # 5) 각 attention head의 결과물을 concatenate해 병합함\n","        attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","        attn_values = attn_values.contiguous().view(batch_size, seq_len, self.d_model)  # (B, L, d_model)\n","\n","        # 6) head 별로 정해진 가중치로 linear projection하여 최종 출력을 결정함\n","        outputs = self.w_o(attn_values)  # (B, L, d_model)\n","\n","        return outputs\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        ################################################################################\n","        #                                 END OF YOUR CODE                             #\n","        ################################################################################"],"metadata":{"id":"Kf_2JwweqsLB","executionInfo":{"status":"ok","timestamp":1724994245290,"user_tz":-540,"elapsed":345,"user":{"displayName":"신혜민","userId":"17175468705928178146"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["multi_attn = MultiheadAttention(d_model, d_k, num_heads)\n","outputs = multi_attn(batch_emb)  # (B, L, d_model)\n","\n","print(outputs)\n","print(outputs.shape)"],"metadata":{"id":"xqPgzJwlq18r","executionInfo":{"status":"ok","timestamp":1724994253443,"user_tz":-540,"elapsed":317,"user":{"displayName":"신혜민","userId":"17175468705928178146"}},"outputId":"bf8f25e5-00cc-4532-f335-23dd70c39b32","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.1000,  0.0929,  0.1080,  ..., -0.0644,  0.0509,  0.1925],\n","         [-0.0399,  0.0436,  0.1389,  ..., -0.0301,  0.0496,  0.1690],\n","         [-0.0580,  0.0631,  0.1083,  ...,  0.0279,  0.1052,  0.1502],\n","         ...,\n","         [-0.0502,  0.1020,  0.0912,  ..., -0.0265,  0.0589,  0.1189],\n","         [-0.0502,  0.1020,  0.0912,  ..., -0.0265,  0.0589,  0.1189],\n","         [-0.0502,  0.1020,  0.0912,  ..., -0.0265,  0.0589,  0.1189]],\n","\n","        [[-0.2198,  0.1510, -0.1137,  ..., -0.3374,  0.2957,  0.4713],\n","         [-0.1906,  0.1591, -0.0483,  ..., -0.3505,  0.3057,  0.4240],\n","         [-0.2532,  0.1640, -0.0428,  ..., -0.3516,  0.3230,  0.4309],\n","         ...,\n","         [-0.1612,  0.1964, -0.0998,  ..., -0.2925,  0.2985,  0.4221],\n","         [-0.1612,  0.1964, -0.0998,  ..., -0.2925,  0.2985,  0.4221],\n","         [-0.1612,  0.1964, -0.0998,  ..., -0.2925,  0.2985,  0.4221]],\n","\n","        [[-0.1423,  0.0851,  0.0617,  ..., -0.0927,  0.0178,  0.2986],\n","         [-0.0787,  0.1375,  0.0315,  ..., -0.0685, -0.0454,  0.2647],\n","         [-0.1502,  0.1134,  0.0301,  ..., -0.1232, -0.0010,  0.2466],\n","         ...,\n","         [-0.0636,  0.1048, -0.0238,  ..., -0.0857, -0.0274,  0.3044],\n","         [-0.0636,  0.1048, -0.0238,  ..., -0.0857, -0.0274,  0.3044],\n","         [-0.0636,  0.1048, -0.0238,  ..., -0.0857, -0.0274,  0.3044]],\n","\n","        ...,\n","\n","        [[ 0.0197,  0.0133,  0.0260,  ..., -0.0742, -0.1038,  0.0450],\n","         [-0.0060, -0.0193,  0.0477,  ..., -0.0035, -0.1214,  0.1023],\n","         [-0.0202,  0.0460, -0.0013,  ...,  0.0007, -0.1086,  0.1184],\n","         ...,\n","         [-0.0191,  0.1012,  0.0251,  ..., -0.0228, -0.1409,  0.1374],\n","         [-0.0210,  0.0138,  0.0189,  ...,  0.0175, -0.1069,  0.1408],\n","         [-0.0142,  0.0106, -0.0095,  ..., -0.0364, -0.1119,  0.1153]],\n","\n","        [[ 0.1437,  0.0328, -0.0244,  ..., -0.0943,  0.0235,  0.1372],\n","         [ 0.1073,  0.0717,  0.0126,  ..., -0.0932,  0.0313,  0.1182],\n","         [ 0.1382,  0.0807, -0.0378,  ..., -0.0662,  0.0278,  0.0983],\n","         ...,\n","         [ 0.1854,  0.0613, -0.0152,  ..., -0.0505, -0.0136,  0.1274],\n","         [ 0.1854,  0.0613, -0.0152,  ..., -0.0505, -0.0136,  0.1274],\n","         [ 0.1854,  0.0613, -0.0152,  ..., -0.0505, -0.0136,  0.1274]],\n","\n","        [[-0.0603,  0.1031,  0.0813,  ..., -0.1093,  0.0005,  0.2332],\n","         [-0.0768,  0.1416,  0.0468,  ..., -0.1556,  0.0340,  0.2903],\n","         [-0.0936,  0.1049,  0.0745,  ..., -0.1881,  0.0132,  0.1813],\n","         ...,\n","         [ 0.0007,  0.1055, -0.0128,  ..., -0.0820, -0.0230,  0.2140],\n","         [ 0.0007,  0.1055, -0.0128,  ..., -0.0820, -0.0230,  0.2140],\n","         [ 0.0007,  0.1055, -0.0128,  ..., -0.0820, -0.0230,  0.2140]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([10, 20, 512])\n"]}]}]}